{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3e109a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d6f6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msalvetti/notebooks_2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-17 15:35:27.697644: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import os, sys\n",
    "\n",
    "modules = os.path.abspath(os.path.join(\"..\", \"src\"))\n",
    "sys.path.append(modules)\n",
    "\n",
    "import polars as pl\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import nlp_utils, keywords_utils, load_utils, run_analysis\n",
    "from config import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56539a9",
   "metadata": {},
   "source": [
    "# Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63f4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ARGUS_chunk_p183.parquet is corrupted\n"
     ]
    }
   ],
   "source": [
    "lf = load_utils.load_parquets(\"/home/msalvetti/KOFScraper/chunks/run_id=2025-11-21/parsed\").select(\"ID\", \"url\", \"text\")\n",
    "\n",
    "MAX_CHARS = 10 * 1024\n",
    "lf = lf.filter(pl.col(\"text\") != \"\").with_columns(\n",
    "    [pl.when(pl.col(\"text\").str.len_chars() > MAX_CHARS)\n",
    "     .then(pl.col(\"text\").str.slice(0, MAX_CHARS))\n",
    "     .otherwise(pl.col(\"text\")).alias(\"text\")]\n",
    ")\n",
    "lf = lf.filter(pl.col(\"text\")!= \"\")\\\n",
    ".filter(~pl.col(\"text\").str.starts_with(\"Powered by Imunify360\")).unique(\"text\")\\\n",
    ".filter(~pl.col(\"text\").str.contains(\"Loader Please wait while your request is being verified...\"))\\\n",
    ".filter(~pl.col(\"text\").str.contains(\"You need to enable JavaScript to run this app\"))\\\n",
    ".filter(~pl.col(\"text\").str.contains(\"flex-basis: \"))\\\n",
    ".filter(~pl.col(\"text\").str.contains(\"document.getElementById\"))\n",
    "\n",
    "lf.sink_parquet(pl.PartitionMaxSize(\"split_dataset/\", max_size=3000), mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773c4204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2.6%, (10 / 391)\n",
      "ETA: 0:22:29.241999\n",
      "Memory usage: 11695.1484375 MB\n",
      "Done file output/00000106.parquet\n",
      "Done 5.1%, (20 / 391)\n",
      "ETA: 0:21:47.517504\n",
      "Memory usage: 11705.70703125 MB\n",
      "Done file output/0000012a.parquet\n",
      "Done 7.7%, (30 / 391)\n",
      "ETA: 0:21:48.630623\n",
      "Memory usage: 11661.01953125 MB\n",
      "Done file output/00000055.parquet\n",
      "Done 10.2%, (40 / 391)\n",
      "ETA: 0:20:47.259243\n",
      "Memory usage: 11678.9453125 MB\n",
      "Done file output/00000000.parquet\n",
      "Done 12.8%, (50 / 391)\n",
      "ETA: 0:20:10.132223\n",
      "Memory usage: 11676.15234375 MB\n",
      "Done file output/000000c5.parquet\n",
      "Done 15.3%, (60 / 391)\n",
      "ETA: 0:19:31.057383\n",
      "Memory usage: 11680.83984375 MB\n",
      "Done file output/000000f8.parquet\n",
      "Done 17.9%, (70 / 391)\n",
      "ETA: 0:18:43.938800\n",
      "Memory usage: 11681.8125 MB\n",
      "Done file output/00000130.parquet\n",
      "Done 20.5%, (80 / 391)\n",
      "ETA: 0:18:00.093517\n",
      "Memory usage: 11682.23046875 MB\n",
      "Done file output/0000011e.parquet\n",
      "Done 23.0%, (90 / 391)\n",
      "ETA: 0:17:19.070963\n",
      "Memory usage: 11690.375 MB\n",
      "Done file output/00000138.parquet\n",
      "Done 25.6%, (100 / 391)\n",
      "ETA: 0:16:40.205869\n",
      "Memory usage: 11695.33984375 MB\n",
      "Done file output/0000002e.parquet\n",
      "Done 28.1%, (110 / 391)\n",
      "ETA: 0:16:05.335570\n",
      "Memory usage: 11738.1328125 MB\n",
      "Done file output/0000017a.parquet\n",
      "Done 30.7%, (120 / 391)\n",
      "ETA: 0:15:26.985984\n",
      "Memory usage: 11739.16015625 MB\n",
      "Done file output/000000a3.parquet\n",
      "Done 33.2%, (130 / 391)\n",
      "ETA: 0:14:49.452348\n",
      "Memory usage: 11739.109375 MB\n",
      "Done file output/00000162.parquet\n",
      "Done 35.8%, (140 / 391)\n",
      "ETA: 0:14:13.436664\n",
      "Memory usage: 11738.47265625 MB\n",
      "Done file output/00000090.parquet\n",
      "Done 38.4%, (150 / 391)\n",
      "ETA: 0:13:38.830392\n",
      "Memory usage: 11739.6953125 MB\n",
      "Done file output/000000d0.parquet\n",
      "Done 40.9%, (160 / 391)\n",
      "ETA: 0:13:03.257757\n",
      "Memory usage: 11738.74609375 MB\n",
      "Done file output/0000013e.parquet\n",
      "Done 43.5%, (170 / 391)\n",
      "ETA: 0:12:27.696361\n",
      "Memory usage: 11740.74609375 MB\n",
      "Done file output/000000c3.parquet\n",
      "Done 46.0%, (180 / 391)\n",
      "ETA: 0:11:52.110705\n",
      "Memory usage: 11738.98046875 MB\n",
      "Done file output/00000150.parquet\n",
      "Done 48.6%, (190 / 391)\n",
      "ETA: 0:11:16.414527\n",
      "Memory usage: 11738.14453125 MB\n",
      "Done file output/00000104.parquet\n",
      "Done 51.2%, (200 / 391)\n",
      "ETA: 0:10:42.121836\n",
      "Memory usage: 11741.66015625 MB\n",
      "Done file output/00000171.parquet\n",
      "Done 53.7%, (210 / 391)\n",
      "ETA: 0:10:09.750026\n",
      "Memory usage: 11773.5078125 MB\n",
      "Done file output/00000118.parquet\n",
      "Done 56.3%, (220 / 391)\n",
      "ETA: 0:09:36.044782\n",
      "Memory usage: 11575.4296875 MB\n",
      "Done file output/0000001b.parquet\n",
      "Done 58.8%, (230 / 391)\n",
      "ETA: 0:09:02.240264\n",
      "Memory usage: 11575.2734375 MB\n",
      "Done file output/000000cd.parquet\n",
      "Done 61.4%, (240 / 391)\n",
      "ETA: 0:08:29.655410\n",
      "Memory usage: 11311.734375 MB\n",
      "Done file output/0000007e.parquet\n",
      "Done 63.9%, (250 / 391)\n",
      "ETA: 0:07:59.573805\n",
      "Memory usage: 11240.90625 MB\n",
      "Done file output/0000002b.parquet\n",
      "Done 66.5%, (260 / 391)\n",
      "ETA: 0:07:28.874799\n",
      "Memory usage: 10667.21484375 MB\n",
      "Done file output/000000fc.parquet\n",
      "Done 69.1%, (270 / 391)\n",
      "ETA: 0:06:56.508891\n",
      "Memory usage: 10692.203125 MB\n",
      "Done file output/0000001e.parquet\n",
      "Done 71.6%, (280 / 391)\n",
      "ETA: 0:06:22.388934\n",
      "Memory usage: 10707.8828125 MB\n",
      "Done file output/00000135.parquet\n",
      "Done 74.2%, (290 / 391)\n",
      "ETA: 0:05:48.262179\n",
      "Memory usage: 10716.8984375 MB\n",
      "Done file output/0000015a.parquet\n",
      "Done 76.7%, (300 / 391)\n",
      "ETA: 0:05:13.971830\n",
      "Memory usage: 10720.32421875 MB\n",
      "Done file output/0000000d.parquet\n",
      "Done 79.3%, (310 / 391)\n",
      "ETA: 0:04:39.901570\n",
      "Memory usage: 10735.328125 MB\n",
      "Done file output/000000ba.parquet\n",
      "Done 81.8%, (320 / 391)\n",
      "ETA: 0:04:05.796000\n",
      "Memory usage: 10737.6953125 MB\n",
      "Done file output/00000185.parquet\n",
      "Done 84.4%, (330 / 391)\n",
      "ETA: 0:03:31.147412\n",
      "Memory usage: 10738.15234375 MB\n",
      "Done file output/000000e0.parquet\n",
      "Done 87.0%, (340 / 391)\n",
      "ETA: 0:02:56.361945\n",
      "Memory usage: 10740.21484375 MB\n",
      "Done file output/0000003b.parquet\n",
      "Done 89.5%, (350 / 391)\n",
      "ETA: 0:02:21.962699\n",
      "Memory usage: 10740.40234375 MB\n",
      "Done file output/0000017e.parquet\n",
      "Done 92.1%, (360 / 391)\n",
      "ETA: 0:01:47.335919\n",
      "Memory usage: 10740.80078125 MB\n",
      "Done file output/00000012.parquet\n",
      "Done 94.6%, (370 / 391)\n",
      "ETA: 0:01:12.702987\n",
      "Memory usage: 10750.9453125 MB\n",
      "Done file output/00000053.parquet\n",
      "Done 97.2%, (380 / 391)\n",
      "ETA: 0:00:38.082021\n",
      "Memory usage: 10752.21484375 MB\n",
      "Done file output/00000021.parquet\n",
      "Done 99.7%, (390 / 391)\n",
      "ETA: 0:00:03.463888\n",
      "Memory usage: 10751.3515625 MB\n",
      "Done file output/000000aa.parquet\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import psutil\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "\n",
    "MODEL_ID = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "MODEL_ID = \"google/embeddinggemma-300m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer(t, padding = 'max_length', max_length = 1024, truncation = True, return_tensors = \"pt\").data\n",
    "    \n",
    "\n",
    "mem: List[str] = []\n",
    "\n",
    "i = 0\n",
    "out_path = Path(\"out_gemma\")\n",
    "out_path.mkdir(exist_ok=True)\n",
    "n_files = len(glob.glob(\"output/000*.parquet\"))\n",
    "start = time.time()\n",
    "\n",
    "with open(\"memory_usage.txt\", \"w+\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "for path in glob.glob(\"output/000*.parquet\"):\n",
    "    i += 1\n",
    "    df = pl.read_parquet(path).unique(\"text\")\n",
    "    texts = df[\"text\"].to_list()\n",
    "    urls = df[\"url\"].to_list()\n",
    "    toks = tokenize(texts)\n",
    "\n",
    "    table = pl.DataFrame(toks)\n",
    "    table = table.with_columns(pl.Series(name = \"url\", values = urls))\n",
    "    out_name = out_path / f\"tokens_{i:09d}.parquet\"\n",
    "    table.write_parquet(out_name)\n",
    "    end = time.time()\n",
    "    eta = timedelta(seconds = (((end - start) / (i/n_files)) - (end - start)))\n",
    "    process = psutil.Process()\n",
    "    mem.append(process.memory_info().rss / 1024 / 1024)\n",
    "\n",
    "    if (i%10 == 0):\n",
    "        print(f\"Done {(i*100/n_files):.1f}%, ({i} / {n_files})\")\n",
    "        print(f\"ETA: {eta}\")\n",
    "        print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024} MB\")\n",
    "        print(f'Done file {path}')\n",
    "    \n",
    "    if (i%100 == 0): \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding = 'max_length', max_length = 1024, truncation = True, use_fast = True)\n",
    "        with open(\"memory_usage.txt\", \"a\") as f:\n",
    "            f.write(\",\".join(str(m) for m in mem))\n",
    "            mem = []\n",
    "\n",
    "    del table, texts, toks\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc588f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Time')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"memory_usage.txt\") as f:\n",
    "    mem = f.read()\n",
    "mem = mem.split(\",\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"./style.mplstyle\")\n",
    "plot = pd.DataFrame({\"mem\": mem})\n",
    "\n",
    "plot[\"time\"] = plot.index\n",
    "plot[\"time\"] = pd.to_numeric(plot[\"time\"], errors=\"coerce\")\n",
    "plot[\"mem\"]  = pd.to_numeric(plot[\"mem\"], errors=\"coerce\")\n",
    "plot[\"mem\"] = plot[\"mem\"] / 1024\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = plt.subplot()\n",
    "plt.plot(\"time\", \"mem\", data = plot)\n",
    "ax.set_xlim(0)\n",
    "ax.set_ylim(0,30.5)\n",
    "ax.set_xticks([])\n",
    "# ax.xaxis.set_major_locator(MultipleLocator(100))\n",
    "# ax.xaxis.set_minor_locator(MultipleLocator(50))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(2))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "ax.plot(1, 0, \">k\", transform=ax.get_yaxis_transform(), clip_on = False)\n",
    "ax.fill_between(x = \"time\", y1=\"mem\", y2=0, data = plot, alpha = 0.2)\n",
    "plt.ylabel(\"Memory in GBs\")\n",
    "plt.xlabel(\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9528fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "lf = pl.scan_parquet(\"out/tokens*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a7a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = lf.select('input_ids').collect(engine = \"streaming\")[\"input_ids\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a94ed3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.get_input_embeddings()\n",
    "\n",
    "def get_tokens(lf, chunk_size):\n",
    "    n_rows = lf.select(pl.len()).collect(engine = \"streaming\").item()\n",
    "    for i in range(0, n_rows, chunk_size):\n",
    "        tokens = lf.slice(i, chunk_size).select(\"input_ids\").collect(engine = \"streaming\")[\"input_ids\"].to_list()\n",
    "        embs = embed(torch.tensor(tokens, dtype = torch.long))\n",
    "        frame = pl.DataFrame({'embedding': embs})\n",
    "        frame.write_parquet(f\"embedding_{i}.parequt\")\n",
    "\n",
    "        del tokens, embs, frame\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455721b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "f = pq.read_table(\"output/00000000.parquet\")\n",
    "texts = f.to_pydict()[\"text\"]\n",
    "\n",
    "MODEL_ID = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer(t, padding = 'max_length', max_length = 512, truncation = True, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e1e6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47380e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaa8f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m model = AutoModel.from_pretrained(MODEL_ID)\n\u001b[32m      5\u001b[39m embed = model.get_input_embeddings()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m toks_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m embs = embed(toks_tensor)\n",
      "\u001b[31mTypeError\u001b[39m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0787192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"out/tokens_000000001.parquet\")[\"input_ids\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38d608c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m enc = tokenizer(texts,return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation = \u001b[38;5;28;01mTrue\u001b[39;00m, padding = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:724\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    720\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    721\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    722\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:531\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    529\u001b[39m     all_hidden_states = all_hidden_states + (hidden_state,)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:466\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    463\u001b[39m \u001b[33;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m sa_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    475\u001b[39m     sa_output, sa_weights = sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:392\u001b[39m, in \u001b[36mDistilBertSdpaAttention.forward\u001b[39m\u001b[34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    389\u001b[39m k = shape(\u001b[38;5;28mself\u001b[39m.k_lin(key))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[32m    390\u001b[39m v = shape(\u001b[38;5;28mself\u001b[39m.v_lin(value))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m attn_output = unshape(attn_output)\n\u001b[32m    402\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.out_lin(attn_output)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "f = pq.read_table(\"output/00000000.parquet\")\n",
    "texts = f.to_pydict()[\"text\"]\n",
    "\n",
    "enc = tokenizer(texts,return_tensors=\"pt\", truncation = True, padding = True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(**enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79400b4b",
   "metadata": {},
   "source": [
    "# Make some order\n",
    "\n",
    "Basically I want the whole output of the tokenizer, then pass it to model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa498b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "f = pq.read_table(\"output/00000000.parquet\")\n",
    "texts = f.to_pydict()[\"text\"][0:10]\n",
    "\n",
    "MODEL_ID = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer(t, padding = 'max_length', max_length = 512, truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "toks = tokenize(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9eb8a",
   "metadata": {},
   "source": [
    "At this point we have tokens, now we need embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2623f13",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m toks = transformers.tokenization_utils_base.BatchEncoding({\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: input_ids, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: attention_mask})\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtoks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m sentence_embeddings = output.last_hidden_state[:, \u001b[32m0\u001b[39m, :].numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:724\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    720\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    721\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    722\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:531\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    529\u001b[39m     all_hidden_states = all_hidden_states + (hidden_state,)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:484\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    481\u001b[39m sa_output = \u001b[38;5;28mself\u001b[39m.sa_layer_norm(sa_output + x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m ffn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    485\u001b[39m ffn_output: torch.Tensor = \u001b[38;5;28mself\u001b[39m.output_layer_norm(ffn_output + sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    487\u001b[39m output = (ffn_output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:418\u001b[39m, in \u001b[36mFFN.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:423\u001b[39m, in \u001b[36mFFN.ff_chunk\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    421\u001b[39m x = \u001b[38;5;28mself\u001b[39m.lin1(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    422\u001b[39m x = \u001b[38;5;28mself\u001b[39m.activation(x)\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlin2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks_2/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_ID)\n",
    "input_ids = torch.tensor(pl.read_parquet(\"out/tokens_000000001.parquet\").to_dict()[\"input_ids\"])\n",
    "attention_mask = torch.tensor(pl.read_parquet(\"out/tokens_000000001.parquet\").to_dict()[\"attention_mask\"])\n",
    "toks = transformers.tokenization_utils_base.BatchEncoding({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**toks)\n",
    "\n",
    "sentence_embeddings = output.last_hidden_state[:, 0, :].numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5b54f",
   "metadata": {},
   "source": [
    "Ok this works. Now we only want to write the tokens to a file and be able to retrieve them as a transformers.tokenization_utils_base.BatchEncoding type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "664fd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pl.read_parquet(\"out/tokens_000000001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e9a24b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(pl.read_parquet(\"out/tokens_000000001.parquet\").to_dict()[\"input_ids\"])\n",
    "torch.tensor(pl.read_parquet(\"out/tokens_000000001.parquet\").to_dict()[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5120fa2",
   "metadata": {},
   "source": [
    "# ASPI categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe1834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 14:14:39.782426: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import os, sys\n",
    "\n",
    "modules = os.path.abspath(os.path.join(\"..\", \"src\"))\n",
    "sys.path.append(modules)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import nlp_utils\n",
    "from config import *\n",
    "\n",
    "categories = pl.read_parquet(DATA_DIR + \"processed/aspi_categories.parquet\")[\"description\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42c769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer(t, padding = 'max_length', max_length = 512, truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "toks = tokenize(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a619887b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (73, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>input_ids</th><th>attention_mask</th></tr><tr><td>array[i64, 512]</td><td>array[i64, 512]</td></tr></thead><tbody><tr><td>[101, 26745,  0]</td><td>[1, 1,  0]</td></tr><tr><td>[101, 26745,  0]</td><td>[1, 1,  0]</td></tr><tr><td>[101, 26745,  0]</td><td>[1, 1,  0]</td></tr><tr><td>[101, 38392,  0]</td><td>[1, 1,  0]</td></tr><tr><td>[101, 13828,  0]</td><td>[1, 1,  0]</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>[101, 65136,  0]</td><td>[1, 1,  0]</td></tr><tr><td>[101, 42491,  0]</td><td>[1, 1,  0]</td></tr><tr><td>[101, 12395,  0]</td><td>[1, 1,  0]</td></tr><tr><td>[101, 11704,  0]</td><td>[1, 1,  0]</td></tr><tr><td>[101, 84508,  0]</td><td>[1, 1,  0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (73, 2)\n",
       "\n",
       " input_ids          attention_mask  \n",
       " ---                ---             \n",
       " array[i64, 512]    array[i64, 512] \n",
       "\n",
       " [101, 26745,  0]  [1, 1,  0]     \n",
       " [101, 26745,  0]  [1, 1,  0]     \n",
       " [101, 26745,  0]  [1, 1,  0]     \n",
       " [101, 38392,  0]  [1, 1,  0]     \n",
       " [101, 13828,  0]  [1, 1,  0]     \n",
       "                                  \n",
       " [101, 65136,  0]  [1, 1,  0]     \n",
       " [101, 42491,  0]  [1, 1,  0]     \n",
       " [101, 12395,  0]  [1, 1,  0]     \n",
       " [101, 11704,  0]  [1, 1,  0]     \n",
       " [101, 84508,  0]  [1, 1,  0]     \n",
       ""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.DataFrame(toks.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d122518",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pl.DataFrame(toks.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dec8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.write_parquet(\"tokens_aspi.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
